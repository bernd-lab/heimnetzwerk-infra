# Ressourcen-Analyse - 2025-11-09

## üìä Zusammenfassung

**Tats√§chliche Auslastung vs. Limits:**

| Metrik | Tats√§chlich | Requests | Limits | Status |
|--------|-------------|----------|--------|--------|
| **CPU** | 1123m (28%) | 4000m (100%) ‚ö†Ô∏è | 14400m (360%) | ‚úÖ OK |
| **Memory** | 12368Mi (38%) | 27476Mi (86%) | 39296Mi (123%) | ‚úÖ OK |

---

## üîç Detaillierte Analyse

### 1. Tats√§chliche Nutzung (kubectl top)

**CPU:**
- **Gesamt**: 1123m von 4000m verf√ºgbar = **28% Auslastung**
- **Top-Verbraucher**:
  - GitLab: 192m (4.8%)
  - kube-apiserver: 107m (2.7%)
  - etcd: 55m (1.4%)
  - gitlab-redis: 40m (1.0%)

**Memory:**
- **Gesamt**: 12368Mi von ~32GB verf√ºgbar = **38% Auslastung**
- **Top-Verbraucher**:
  - GitLab: 3242Mi (10%)
  - kube-apiserver: 673Mi (2%)
  - Komga: 488Mi (1.5%)
  - ArgoCD: 259Mi (0.8%)

### 2. Allocated Resources (Requests)

**CPU Requests:**
- **Gesamt**: 4000m = **100% belegt** ‚ö†Ô∏è
- **Problem**: Alle CPU-Ressourcen sind durch Requests blockiert
- **Auswirkung**: Neue Pods mit CPU-Requests k√∂nnen nicht gestartet werden

**Memory Requests:**
- **Gesamt**: 27476Mi = **86% belegt**
- **Status**: Noch 14% frei f√ºr neue Pods

### 3. Limits (Overcommitment)

**CPU Limits:**
- **Gesamt**: 14400m = **360% Overcommitment**
- **Bedeutung**: Pods k√∂nnen theoretisch bis zu 14.4 Cores nutzen, aber nur 4 verf√ºgbar
- **Status**: ‚úÖ OK, da tats√§chliche Nutzung nur 28% ist

**Memory Limits:**
- **Gesamt**: 39296Mi = **123% Overcommitment**
- **Bedeutung**: Pods k√∂nnen theoretisch bis zu 39GB nutzen, aber nur ~32GB verf√ºgbar
- **Status**: ‚úÖ OK, da tats√§chliche Nutzung nur 38% ist

---

## ‚ö†Ô∏è Problem-Analyse

### Warum zeigt Pi-hole "Load Average 4.6 > 4"?

**Tats√§chliche Host-Daten (192.168.178.54):**
- **Load Average**: 6.22 (1min), 4.27 (5min), 2.75 (15min) ‚ö†Ô∏è
- **CPU-Nutzung**: 16.7% user, 25.0% system, **58.3% idle** ‚úÖ
- **Memory**: 9.2Gi benutzt von 31Gi = **30% Auslastung** ‚úÖ
- **I/O-Wartezeit**: 0.52% (sehr niedrig) ‚úÖ
- **Prozesse**: 510 Tasks (1 running, 509 sleeping)

**Ursachen der hohen Load Average:**

1. **Viele laufende Prozesse**: 510 Tasks insgesamt, davon viele Kubernetes-Pods und Systemprozesse. Die Load Average misst die Anzahl wartender Prozesse, nicht nur CPU-Nutzung.

2. **Kubernetes Overhead**: 
   - kube-apiserver: 11.2% CPU
   - kubelet: 9.3% CPU
   - etcd: 5.6% CPU
   - Diese Prozesse verursachen Context-Switching, auch wenn die CPU-Nutzung niedrig ist.

3. **GitLab Overhead**: 
   - GitLab Sidekiq: 17.5% CPU
   - GitLab Puma Workers: mehrere Prozesse
   - GitLab Gitaly: l√§uft im Hintergrund

4. **Context Switching**: Bei vielen Prozessen muss der Kernel h√§ufig zwischen Prozessen wechseln, was die Load Average erh√∂ht, auch wenn die CPU nicht voll ausgelastet ist.

### Ist das ein Problem?

**Nein, aktuell kein kritisches Problem:**

- ‚úÖ Alle Pods laufen (0 Pending, 0 Failed)
- ‚úÖ Tats√§chliche CPU-Nutzung ist niedrig (28%)
- ‚úÖ Tats√§chliche Memory-Nutzung ist niedrig (38%)
- ‚úÖ Keine Ressourcen-Engp√§sse bei laufenden Pods
- ‚ö†Ô∏è CPU Requests sind bei 100% belegt (kann neue Pods blockieren)

---

## üí° Empfehlungen

### Kurzfristig (Optional):

1. **CPU Requests reduzieren**: Einige nicht-kritische Pods haben m√∂glicherweise zu hohe CPU-Requests. Diese k√∂nnten reduziert werden, um mehr "Luft" f√ºr neue Pods zu schaffen.

2. **Monitoring**: Die Load Average sollte √ºberwacht werden. Wenn sie dauerhaft √ºber 4 bleibt, k√∂nnte dies auf I/O-Probleme oder andere Engp√§sse hinweisen.

### Langfristig (Optional):

1. **Node erweitern**: Wenn mehr Services hinzugef√ºgt werden sollen, k√∂nnte der Node um mehr CPU-Kerne erweitert werden.

2. **Workloads verteilen**: Wenn m√∂glich, k√∂nnten Workloads auf mehrere Nodes verteilt werden.

---

## üìà Fazit

**Die Maschine ist NICHT wirklich ausgelastet:**

- ‚úÖ Tats√§chliche CPU-Nutzung: **28%** (niedrig)
- ‚úÖ Tats√§chliche Memory-Nutzung: **38%** (niedrig)
- ‚ö†Ô∏è CPU Requests: **100%** (blockiert neue Pods, aber keine Auswirkung auf laufende Pods)
- ‚ö†Ô∏è Load Average: **4.6** (h√∂her als erwartet, aber kein kritisches Problem)

**Die hohe Load Average (6.22) ist auf die gro√üe Anzahl laufender Prozesse (510 Tasks) und Kubernetes Overhead zur√ºckzuf√ºhren, NICHT auf tats√§chliche CPU-Auslastung (CPU ist zu 58% idle).**

**Top-Prozesse nach CPU:**
- GitLab Sidekiq: 17.5% CPU
- kube-apiserver: 11.2% CPU  
- kubelet: 9.3% CPU
- etcd: 5.6% CPU

**Top-Prozesse nach Memory:**
- GitLab Sidekiq: 967MB
- GitLab Puma Workers: ~950MB je Worker
- kube-apiserver: 649MB
- Komga (Java): 524MB

Die Limits sind hoch (Overcommitment), aber das ist OK, da die tats√§chliche Nutzung niedrig ist. Die Pods k√∂nnen bei Bedarf mehr Ressourcen nutzen, solange die Limits nicht √ºberschritten werden.

---

**Ende der Analyse**

