# Kubernetes-Cluster Handover Dokumentation

**Datum**: 2025-11-06  
**Erstellt von**: System-Handover  
**Status**: Vollst√§ndiger Handover f√ºr n√§chsten Agenten

---

## Executive Summary

### Cluster-Status
- **Nodes**: 2 (1 Ready, 1 NotReady)
- **Kubernetes Version**: v1.34.1 (beide Nodes)
- **Pods**: 51 total (48 Running, 3 Pending)
- **Namespaces**: 23 aktiv
- **Services**: 30+ (2 LoadBalancer)
- **Ingress**: 13 aktive Ingress-Ressourcen
- **Storage**: 24 PVCs, 24 PVs (NFS-basiert)

### Kritische Probleme
1. **WSL2-Node NotReady** - Kubelet-Fehler: "system validation failed" (P0)
2. **3 Pods Pending** - Jenkins (2x), Velero (1x) - CPU-Ressourcen-Mangel (P1)

### Funktionierende Services
‚úÖ Pi-hole, ArgoCD, GitLab, Ingress-nginx, CoreDNS, Monitoring, Logging, Komga, Heimdall, Syncthing, Cert-Manager

---

## Cluster-Architektur

### Nodes

| Node | IP | Status | Role | OS | Kubernetes | Container Runtime |
|------|----|----|-----|----|----|----|
| zuhause | 192.168.178.54 | Ready | control-plane | Debian 12 | v1.34.1 | containerd 1.6.20 |
| wsl2-ubuntu | 172.31.16.162 | NotReady | worker | Ubuntu 24.04 WSL2 | v1.34.1 | containerd 1.7.28 |

**Wichtig**: WSL2-Node ist aktuell nicht funktionsf√§hig. Alle Pods laufen auf `zuhause` Node.

### Netzwerk

- **Pod Network**: 10.244.0.0/16 (Flannel CNI)
- **Service Network**: 10.100.0.0/16
- **Control Plane**: https://192.168.178.54:6443
- **CNI**: Flannel (2 DaemonSet Pods)

### Storage

**StorageClasses**:
- `nfs-data` (default) - Dynamische Provisionierung auf `/DATA`
- `nfs-elements` - F√ºr `/media/devmon/Elements`
- `nfs-wd-black` - F√ºr `/media/devmon/WD-Black_8TB`
- `manual` - Manuelle Provisionierung

**NFS-Server** (auf zuhause Node):
- Server: 192.168.178.54
- Exports: `/DATA`, `/media/devmon/Elements`, `/media/devmon/WD-Black_8TB`
- Erlaubte Subnetze: `192.168.178.0/24`, `172.31.16.0/20` (WSL2-Subnetz hinzugef√ºgt)

**PVCs/PVs**: 24 PVCs mit 24 zugeh√∂rigen PVs (alle NFS-basiert)

---

## Services und Namespaces

### LoadBalancer Services

| Service | Namespace | External IP | Ports | Status |
|---------|-----------|-------------|-------|--------|
| ingress-nginx-controller | ingress-nginx | 192.168.178.54 | 80, 443 | ‚úÖ Running |
| pihole | pihole | 192.168.178.10 | 53, 80 | ‚úÖ Running |

### Ingress-Ressourcen (13)

Alle Services sind √ºber `*.k8sops.online` erreichbar:

| Namespace | Service | Host | TLS |
|-----------|--------|------|-----|
| argocd | ArgoCD | argocd.k8sops.online | ‚úÖ |
| default | Jellyfin | jellyfin.k8sops.online | ‚úÖ |
| default | Jenkins | jenkins.k8sops.online | ‚úÖ |
| default | PlantUML | plantuml.k8sops.online | ‚úÖ |
| gitlab | GitLab | gitlab.k8sops.online | ‚úÖ |
| heimdall | Heimdall | heimdall.k8sops.online | ‚úÖ |
| komga | Komga | komga.k8sops.online | ‚úÖ |
| logging | Loki | loki.k8sops.online | ‚úÖ |
| monitoring | Grafana | grafana.k8sops.online | ‚úÖ |
| monitoring | Prometheus | prometheus.k8sops.online | ‚úÖ |
| syncthing | Syncthing | syncthing.k8sops.online | ‚úÖ |
| kubernetes-dashboard | Dashboard | dashboard.k8sops.online | ‚úÖ |
| test-tls | Test | test.k8sops.online | ‚úÖ |

### Namespaces (23)

| Namespace | Pods | Zweck | Status |
|-----------|------|-------|--------|
| argocd | 7 | GitOps Deployment | ‚úÖ Running |
| cert-manager | 3 | TLS-Zertifikate | ‚úÖ Running |
| default | 8 | Standard-Services | ‚ö†Ô∏è 2 Pending |
| gitlab | 3 | GitLab CI/CD | ‚úÖ Running |
| gitlab-agent | 1 | GitLab Agent | ‚úÖ Running |
| gitlab-runner | 1 | GitLab Runner | ‚úÖ Running |
| heimdall | 1 | Dashboard | ‚úÖ Running |
| ingress-nginx | 3 | Ingress Controller | ‚úÖ Running |
| komga | 1 | Manga-Server | ‚úÖ Running |
| kube-flannel | 2 | CNI Plugin | ‚úÖ Running |
| kube-system | 8 | System Components | ‚úÖ Running |
| kubernetes-dashboard | 1 | Dashboard | ‚úÖ Running |
| logging | 1 | Loki Logging | ‚úÖ Running |
| metallb-system | 2 | LoadBalancer | ‚úÖ Running |
| monitoring | 3 | Prometheus/Grafana | ‚úÖ Running |
| pihole | 1 | DNS/Ad-Blocker | ‚úÖ Running |
| syncthing | 1 | File-Sync | ‚úÖ Running |
| test-tls | 1 | TLS Testing | ‚úÖ Running |
| velero | 3 | Backup | ‚ö†Ô∏è 1 Pending |

---

## Kritische Probleme

### 1. WSL2-Node NotReady (P0 - Kritisch)

**Problem**:
- Node `wsl2-ubuntu` ist im Status `NotReady`
- Kubelet-Fehler: "system validation failed - wrong number of fields (expected 6, got 7)"
- Ursache: Kubernetes v1.34.1 Inkompatibilit√§t mit WSL2/cgroup2

**Auswirkung**:
- WSL2-Node kann keine Pods schedulen
- Alle Pods laufen auf `zuhause` Node
- CPU-Ressourcen-Mangel auf `zuhause` Node

**L√∂sungsans√§tze**:
1. **Kubernetes Downgrade** (empfohlen): Auf v1.31.5 downgraden
   - Siehe: `kubernetes-downgrade-feinkonzept.md` (wird erstellt)
2. **WSL2 cgroup v1**: WSL2 auf cgroup v1 umstellen (komplex)
3. **Warten auf Kubernetes-Fix**: F√ºr WSL2/cgroup2 Kompatibilit√§t

**Status**: Wartet auf Implementierung

### 2. Pods Pending - CPU-Ressourcen-Mangel (P1)

**Betroffene Pods**:
- `default/jenkins-6c5c5687f4-z77hf`: Pending
- `default/jenkins-7fb5d89ddf-2rqxf`: Pending
- `velero/velero-7c697f8956-ffphp`: Pending

**Ursache**:
- Alle Pods laufen auf `zuhause` Node (4 CPU)
- CPU-Requests √ºbersteigen verf√ºgbare Ressourcen
- WSL2-Node ist nicht verf√ºgbar

**L√∂sung**:
- WSL2-Node reparieren (siehe Problem 1)
- Oder: CPU-Requests reduzieren f√ºr weniger kritische Services
- Oder: Services auf WSL2-Node verschieben (sobald Ready)

---

## Wichtige Konfigurationen

### DNS

**CoreDNS**:
- Forward an Pi-hole (192.168.178.10) und 8.8.8.8
- Cache: 30 Sekunden
- Service Discovery: cluster.local

**Pi-hole**:
- LoadBalancer IP: 192.168.178.10
- Ports: 53 (TCP/UDP), 80
- Status: ‚úÖ Running

### Ingress-Controller

- **Controller**: nginx-ingress
- **Namespace**: ingress-nginx
- **LoadBalancer IP**: 192.168.178.54
- **Host Network**: true (bindet direkt an Host-IP)
- **ConfigMap**: `allow-snippet-annotations: false` (Sicherheit)

### MetalLB

- **IP-Pool**: `default-pool`
- **Adressen**: `192.168.178.54/32`, `192.168.178.10/32`
- **L2 Advertisement**: aktiviert

### Cert-Manager

- **ClusterIssuer**: `letsencrypt-prod-dns01`
- **Challenge**: DNS01 mit Cloudflare
- **API Token**: Secret `cloudflare-api-token` in `cert-manager` Namespace
- **Status**: ‚úÖ Ready

---

## Bekannte Konfigurationen

### Jellyfin

- **Namespace**: `default` (Deployment), `jellyfin` (Service/Ingress) - **Inkonsistent**
- **PVCs**: Im `default` Namespace
- **Status**: Deployment l√§uft, aber Service/Ingress im anderen Namespace
- **PriorityClass**: `jellyfin-high-priority` (1000000)

### GitLab

- **Namespace**: `gitlab`
- **Ingress**: `gitlab.k8sops.online`
- **TLS**: Cert-Manager Zertifikat
- **Liveness-Probe**: Verwendet `exec` mit `curl` (nicht `httpGet`)
- **Status**: ‚úÖ Running stabil

### ArgoCD

- **Namespace**: `argocd`
- **Ingress**: `argocd.k8sops.online`
- **TLS**: Cert-Manager Zertifikat
- **Status**: ‚úÖ Running (7 Pods)

---

## Wichtige Dateien

### Kubernetes-Manifeste

- `k8s/` - Alle Kubernetes-Manifeste
- `k8s/jellyfin/` - Jellyfin-Konfiguration
- `k8s/pihole/` - Pi-hole-Konfiguration
- `k8s/tools/` - Utility-Jobs und Tools

### Dokumentation

- `CLUSTER-ANALYSE.md` - Detaillierte Cluster-Analyse
- `PROBLEME.md` - Priorisierte Problem-Liste
- `kubernetes-analyse.md` - Kubernetes-Konfiguration
- `docker-kubernetes-migration.md` - Migrationsplan

### Scripts

- `scripts/deploy-pihole.sh` - Pi-hole Deployment
- `scripts/load-secrets.sh` - Secrets laden

---

## N√§chste Schritte

### Sofort (P0)

1. **WSL2-Node reparieren**:
   - Kubernetes Downgrade auf v1.31.5 durchf√ºhren
   - Oder: Alternative L√∂sung f√ºr cgroup2-Problem finden

### Bald (P1)

2. **Pending Pods beheben**:
   - CPU-Ressourcen optimieren
   - Oder: WSL2-Node reparieren und Pods dorthin verschieben

3. **Namespace-Konsistenz**:
   - Jellyfin komplett in `jellyfin` Namespace verschieben
   - Oder: Alles zur√ºck in `default` Namespace

### Sp√§ter (P2)

4. **Monitoring verbessern**:
   - Metrics-Server installieren (fehlt aktuell)
   - Ressourcen-Monitoring erweitern

5. **Backup-Strategie**:
   - Velero-Backups verifizieren
   - Automatische Backups einrichten

---

## Zugriff und Kontakte

### Kubernetes API

- **Endpoint**: `https://192.168.178.54:6443`
- **Config**: `~/.kube/config` oder `~/.kube/config-new-cluster.yaml`
- **Context**: `kubernetes-admin@kubernetes`

### Services

- **Dashboard**: https://dashboard.k8sops.online
- **GitLab**: https://gitlab.k8sops.online
- **ArgoCD**: https://argocd.k8sops.online
- **Grafana**: https://grafana.k8sops.online
- **Pi-hole**: http://192.168.178.10

### SSH-Zugriff

- **zuhause Node**: `ssh bernd@192.168.178.54`
- **SSH Key**: `~/.ssh/infra_ed25519` (verf√ºgbar)

---

## Wichtige Hinweise

1. **WSL2-Node ist aktuell nicht nutzbar** - Alle Pods laufen auf `zuhause` Node
2. **CPU-Ressourcen sind knapp** - 4 CPU auf `zuhause` Node f√ºr alle Services
3. **NFS-Exports wurden erweitert** - WSL2-Subnetz (172.31.16.0/20) ist erlaubt
4. **Metrics-Server fehlt** - `kubectl top` funktioniert nicht
5. **PVCs k√∂nnen nicht verschoben werden** - An PVs gebunden, ReclaimPolicy beachten

---

## Zusammenarbeit mit Agenten

- **k8s-expert**: Kubernetes-Cluster-Management, Services, Ingress
- **debian-server-expert**: Server-Analyse, Docker, KVM, Node-Management
- **dns-expert**: DNS-Konfiguration, Pi-hole, CoreDNS
- **gitops-expert**: ArgoCD, GitOps-Workflows
- **monitoring-expert**: Prometheus, Grafana, Logging

---

**Viel Erfolg beim Weiterarbeiten! üöÄ**
